{"name":"Rayvon","tagline":"Fast, parallel beam-chasing raycaster for virtual reality displays.","body":"_Carnegie Mellon University 15-418 Final Project by Felipe Gomez-Frittelli_\r\n***\r\n# Abstract\r\nThe goal of this project is to build a state-of-the-art GPU raycaster for rendering 3D scenes in real-time. Using advance hardware and driver support I will optimize my raycaster for minimal latency by filling pixels in a display buffer just in advance of scanline reads to a head-mounted display.\r\n\r\n# Background\r\nHead-mounted displays for Virtual Reality require extremely low latency to prevent simulation sickness, but also require extremely high resolution due to the wide field of view and close proximity to the eye. Since displays read from a front buffer one scanline at a time, rendering an entire frame in advance is wasteful and introduces additional latency. New hardware and software will soon enable developers to anticipate scanline reads from the front buffer, opening the possibility of reducing latency by rendering only a small region at a time - a single row or two, or perhaps even mere pixels - just ahead of the current read location.\r\n\r\nConventional 3D rendering via perspective transform will have difficulty taking advantage of this opportunity because whole objects must be projected and rasterized; there is an initial cost incurred in submitting all objects to be transformed, which is the same for a single row as for a full frame. While it is not yet clear that rasterization cannot be adapted for per-scanline rendering, raycasting offers an obvious alternative, since individual rays can be sampled at any time and image pixels filled in any order. Given scanline info, a 'beam-chasing' raytracer could sample primary visibility (and optionally secondary rays) for pixels just ahead of the current read point, chasing (or actually leading) the 'beam' of the read point envisioned as a ray cast out into the scene.\r\n\r\n# The Challenge\r\nReal-time raytracing is an active area of research with many performance obstacles, but recent state-of-the-art techniques are promising. The main problems inhibiting use of raytracing for interactive applications like virtual reality are\r\n1. Efficiently constructing an optimal acceleration structure for ray-polygon intersection tests. This must be performed every frame for dynamic objects, and it is not clear how best to map this problem to parallel hardware, although [recent research by Karras and Aila](https://research.nvidia.com/publication/fast-parallel-construction-high-quality-bounding-volume-hierarchies) demonstrates a promising solution.\r\n2. Performing millions or more ray-scene intersections using this acceleration structure, with sufficient throughput to render multiple frames at interactive rates and (for beam chasing) with minimal latency per ray or small batch of rays. Many techniques have been proposed, and there is [well-accepted if slightly dated analysis](https://code.google.com/p/understanding-the-efficiency-of-ray-traversal-on-gpus/) of best-practices performance, but research continues in new techniques and even new hardware customized to streamline raytracing workloads.\r\nOf primary concern is the fact that rays cast into a scene are often incoherent and take divergent paths in traversal of an acceleration structure. This causes divergent execution, making raytracing particularly difficult to schedule on wide-SIMD GPU's. Fortunately primary visibility rays are typically the most coherent, which bodes well for a fast implementation on current hardware.\r\n\r\nThe first challenge of this project must therefore be to implement a sufficiently fast raycaster for interactive applications, following state-of-the-art practices and supporting at least primary rays for visible surface detection in dynamic scenes. Implementing and optimizing these algorithms on the latest hardware offers interesting space for analysis, and with sufficient performance, opens the door for a beam-chasing adaptation of a full-frame raycaster. The second challenge of this project will be performing this adaptation and tuning the resulting implementation to achieve the closest possible synchronization to a front buffer being read out to a head-mounted display.\r\n\r\n# Resources and Platform\r\nSince tracking front-buffer reads at scanline rates is not a standard feature of current graphics systems, I anticipate support from AMD through hardware and advance drivers for this project. As a result, I will be using OpenCL to implement a parallel raycaster on an AMD GPU using the Mantle API on Windows. This is necessary to support the beam-chasing modification, and makes use of the lowest-level API currently available on the most common platform for current virtual-reality applications (a PC with a high-end GPU rendering to a head-mounted-display such as the Oculus DK2).\r\n\r\n# Goals and Deliverables\r\nThe primary goal of this project is to implement a GPU-parallel raycaster that is sufficiently fast to render simple dynamic scenes to an Oculus DK2. This is the application that I intend to demonstrate at the parallelism competition; the main deliverable will be a clean code base (maintained in this repository) for a general-purpose raycaster to use with VR displays. As a basic performance projection, [Aila and Laine](https://code.google.com/p/understanding-the-efficiency-of-ray-traversal-on-gpus/) measured performance in excess of 40 Mrays/s, which is sufficient to render two 512x512 images at 75 Hz with one ray per pixel; with newer hardware I hope to exceed this value and obtain higher resolutions.\r\nThe second goal of the project is to modify the raycaster to a beam-chasing implementation, and measure the minimal start-to-scanout latency that can be achieved in this fashion. I will also experiment with varying the sampling rate (rays per pixel) across a rendered image, and analyze the quality and performance of reducing sampling rate with increasing distance from a focal point in the image plane. This is a highly desirable optimization for future HMD's with eye-tracking capability that is also difficult to achieve with rasterization, and can be implemented without the beam-chasing component.\r\n\r\nThe main focus of the project is on the beam-chasing optimization, which is highly dependent on support from AMD. Although we are confident this support will be forthcoming, should beam-chasing not prove feasible I will continue to implement the raycaster with non-uniform-sampling and rendering to a DK2; an analysis of state-of-the-art techniques on the newest hardware and performance measurements of their applicability to VR rendering is a desirable alternative deliverable. If all goes well and _time permitting_, I will design a simple framework for defining and automatically launching rendering tasks in the context of a beam-chasing raycaster as an event-driven system.\r\n\r\n# Schedule\r\n* April 3 - 9: Implement basic framework and begin BVH construction.\r\n* April 10 - 16: Optimize BVH construction, implement raycasting.\r\n**Project Checkpoint (04/16):** A working raycaster; will describe current state, additions needed, and path to integrating with Oculus and beam-chasing.\r\n* April 17 - 23: Test rendering simple scenes, integrate with Oculus.\r\n* April 24 - 30: Measure performance on Oculus, implement beam-chasing if available.\r\n* May 1 - 7: Optimize raycasting and beam chasing and implement non-uniform sampling.\r\n* May 8 - 11: Prepare final presentation and report (last performance measurements).","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}