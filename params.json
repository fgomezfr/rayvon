{"name":"Rayvon","tagline":"Fast, parallel beam-chasing raycaster for virtual reality displays.","body":"_Carnegie Mellon University 15-418 Final Project by Felipe Gomez-Frittelli_\r\n\r\n_Note to Judges: This project is pretty far behind; I've only had Mantle access for two weeks, and experienced a lot of problems that prevented me from implementing BVH construction on time, so there's not much to demo. Please move along._\r\n\r\n***\r\n# Abstract\r\nThe goal of this project is to build a state-of-the-art GPU raycaster for rendering 3D scenes in real-time. Using advance hardware and driver support I will optimize my raycaster for minimal latency by filling pixels in a display buffer just in advance of scanline reads to a head-mounted display.\r\n\r\n# Background\r\nHead-mounted displays for Virtual Reality require extremely low latency to prevent simulation sickness, but also require extremely high resolution due to the wide field of view and close proximity to the eye. Since displays read from a front buffer one scanline at a time, rendering an entire frame in advance is wasteful and introduces additional latency. New hardware and software will soon enable developers to anticipate scanline reads from the front buffer, opening the possibility of reducing latency by rendering only a small region at a time - a single row or two, or perhaps even mere pixels - just ahead of the current read location.\r\n\r\nConventional 3D rendering via perspective transform will have difficulty taking advantage of this opportunity because whole objects must be projected and rasterized; there is an initial cost incurred in submitting all objects to be transformed, which is the same for a single row as for a full frame. While it is not yet clear that rasterization cannot be adapted for per-scanline rendering, raycasting offers an obvious alternative, since individual rays can be sampled at any time and image pixels filled in any order. Given scanline info, a 'beam-chasing' raytracer could sample primary visibility (and optionally secondary rays) for pixels just ahead of the current read point, chasing (or actually leading) the 'beam' of the read point envisioned as a ray cast out into the scene.\r\n\r\n# The Challenge\r\nReal-time raytracing is an active area of research with many performance obstacles, but recent state-of-the-art techniques are promising. The main problems inhibiting use of raytracing for interactive applications like virtual reality are\r\n1. Efficiently constructing an optimal acceleration structure for ray-polygon intersection tests. This must be performed every frame for dynamic objects, and it is not clear how best to map this problem to parallel hardware, although [recent research by Karras and Aila](https://research.nvidia.com/publication/fast-parallel-construction-high-quality-bounding-volume-hierarchies) demonstrates a promising solution.\r\n2. Performing millions or more ray-scene intersections using this acceleration structure, with sufficient throughput to render multiple frames at interactive rates and (for beam chasing) with minimal latency per ray or small batch of rays. Many techniques have been proposed, and there is [well-accepted if slightly dated analysis](https://code.google.com/p/understanding-the-efficiency-of-ray-traversal-on-gpus/) of best-practices performance, but research continues in new techniques and even new hardware customized to streamline raytracing workloads.\r\nOf primary concern is the fact that rays cast into a scene are often incoherent and take divergent paths in traversal of an acceleration structure. This causes divergent execution, making raytracing particularly difficult to schedule on wide-SIMD GPU's. Fortunately primary visibility rays are typically the most coherent, which bodes well for a fast implementation on current hardware.\r\n\r\nThe first challenge of this project must therefore be to implement a sufficiently fast raycaster for interactive applications, following state-of-the-art practices and supporting at least primary rays for visible surface detection in dynamic scenes. Implementing and optimizing these algorithms on the latest hardware offers interesting space for analysis, and with sufficient performance, opens the door for a beam-chasing adaptation of a full-frame raycaster. The second challenge of this project will be performing this adaptation and tuning the resulting implementation to achieve the closest possible synchronization to a front buffer being read out to a head-mounted display.\r\n\r\n# Resources and Platform\r\nSince tracking front-buffer reads at scanline rates is not a standard feature of current graphics systems, I anticipate support from AMD through hardware and advance drivers for this project. As a result, I will be using HLSL compute shaders to implement a parallel raycaster on an AMD GPU using the Mantle API on Windows. This is necessary to support the beam-chasing modification, and makes use of the lowest-level API currently available on the most common platform for current virtual-reality applications (a PC with a high-end GPU rendering to a head-mounted-display such as the Oculus DK2).\r\n\r\n# Goals and Deliverables\r\nThe primary goal of this project is to implement a GPU-parallel raycaster that is sufficiently fast to render simple dynamic scenes to ~~an Oculus DK2~~ _a high-refresh-rate display_. This is the application that I intend to demonstrate at the parallelism competition; the main deliverable will be a clean code base (maintained in this repository) for a general-purpose raycaster ~~to use with VR displays~~. As a basic performance projection, [Aila and Laine](https://code.google.com/p/understanding-the-efficiency-of-ray-traversal-on-gpus/) measured performance in excess of 40 Mrays/s, which is sufficient to render two 512x512 images at 75 Hz with one ray per pixel; with newer hardware I hope to exceed this value and obtain higher resolutions.\r\nThe second goal of the project is to modify the raycaster to a beam-chasing implementation, and measure the minimal start-to-scanout latency that can be achieved in this fashion. I will also experiment with varying the sampling rate (rays per pixel) across a rendered image, and analyze the quality and performance of reducing sampling rate with increasing distance from a focal point in the image plane. This is a highly desirable optimization for future HMD's with eye-tracking capability that is also difficult to achieve with rasterization, and can be implemented without the beam-chasing component.\r\n\r\nThe main focus of the project is on the beam-chasing optimization, which is highly dependent on support from AMD. ~~Although we are confident this support will be forthcoming, should beam-chasing not prove feasible I will continue to implement the raycaster with non-uniform-sampling and rendering to a DK2; an analysis of state-of-the-art techniques on the newest hardware and performance measurements of their applicability to VR rendering is a desirable alternative deliverable.~~ If all goes well and _time permitting_, I will design a simple framework for defining and automatically launching rendering tasks in the context of a beam-chasing raycaster as an event-driven system.\r\n\r\n# Schedule\r\n* April 3 - 9: ~~Implement basic framework and begin BVH construction.~~ __Researched state-of-the-art techniques and Mantle API__\r\n* April 10 - 16: ~~Optimize BVH construction, implement raycasting.~~ __Implemented scene handling and basic raycasting systems for D3D11__\r\n**Project Checkpoint (04/16):** ~~A working raycaster; will describe current state, additions needed, and path to integrating with Oculus and beam-chasing.~~ __Complete: Scene handling, full screen ray generation, unit triangle intersections; In Progress: Raycasting to fill position buffer, basic shading from directional light__\r\n* April 17 - 23: ~~Test rendering simple scenes, integrate with Oculus.~~ __Complete BVH-accelerated raycasting in D3D11__\r\n* April 24 - 30: ~~Measure performance on Oculus, implement beam-chasing if available.~~ __Experiment with possible pipelines in Mantle, identify optimal strategies for sampling blocks of rows in sequence__\r\n* May 1 - 7: Optimize raycasting and beam chasing and implement non-uniform sampling.\r\n* May 8 - 11: Prepare final presentation and report (last performance measurements).\r\n\r\n# PROJECT UPDATE 04/20/2015\r\nOne fact which quickly became apparent during initial research is that the Oculus SDK does not currently support Mantle rendering. Since I probably don't have time to do the integration myself, I'm changing Oculus support to a stretch goal for this project. If I have time I will investigate Mantle-Oculus rendering further in the final week, but I can better devote that time to creating more compelling demos on a desktop monitor (3840x2160 @ 60Hz or 1920x1080 @ 144Hz maximum).\r\nAs of this writing, Mantle development is not yet available to me, but should be within the next few days. This has slowed down development, as I try to read ahead in the Mantle documentation while building against D3D11. By tomorrow I should have full-screen raytracing and simple shading in DirectX. I am now aiming to complete a BVH implementation by next week, so that I can fully devote the last two weeks to exploring the fastest possible implementations in Mantle. BVH construction, and raycasting through the BVH, is the last major but relatively straightforward programming prerequisite for this project, on which I am simply behind schedule due to other commitments.\r\nAt this point, my main concern is moving from DirectX to Mantle; while much of the core code is portable, the new API presents technical challenges for reworking asset and rendering pipelines (easily handled), but also raises interesting questions about memory binding and compute dispatch synchronization, which at this point I cannot resolve without experimentation. I am currently focused on finishing the general coding prerequisites for accelerated raycasting and testing them in a simplistic DirectX renderer, so I can devote plenty of time to Mantle in the last two weeks of the project.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}